Create the Kubernetes cluster
The Kops CLI can be used to create a highly available cluster, with multiple master nodes spread across multiple Availability Zones. Workers can be spread across multiple zones as well. Some of the tasks that happen behind the scene during cluster creation are:
    • Provisioning EC2 instances
    • Setting up AWS resources such as networks, Auto Scaling groups, IAM users, and security groups
    • Installing Kubernetes.
Initiate the Kubernetes cluster creation using the following command:
$ kops create cluster --cloud=aws --zones=us-east-1d --name=awspoc.tk --dns-zone=awspoc.tk --dns=private --vpc=vpc-b2765dc9 --topology=private --subnets=subnet-0151be78e72135f0d --networking calico 
Note: Legends used in the above command described as below:
        ◦ --zones=Defines the zones in which the cluster is going to be created. Multiple comma-separated zones can be specified to span the cluster across multiple zones.
        ◦ --name=Defines the cluster’s name.
        ◦ --state=Points to the S3 bucket that is the state store.
        ◦ --dns private=If you are using a private hosted zone.
        ◦ --yes=Immediately creates the cluster. Otherwise, only the cloud resources are created and the cluster needs to be started explicitly using the command kops update --yes. If the cluster needs to be edited, then the kops edit cluster command can be used.
        ◦ --networking calico=pod cidr networking managed by the networking plugin calico
        ◦ --vpc=The vpc range provided
        ◦ --subnets=The private subnets provided inside which the cluster will exist.
This starts a single master and two worker node Kubernetes cluster. 
    • The master is in an Auto Scaling group and the worker nodes are in a separate group. 
    • By default, the master node is m3.medium and the worker node is t2.medium. 
    • Master and worker nodes are assigned separate IAM roles as well.
Now to actually create cluster run the following command:
$ kops update cluster awspoc.tk –yes
Note: awspoc.tk is used as the cluster name
This creates the autoscaling-groups, nodes etc. which can be observed in the output console. 
To review what all things going to happen while the above command is running; its needed to run the above command without –yes option. Without –yes option, will keep printing the action to be performed without actually doing it. (i.e. perform a dry run)
The cluster components (i.e. Nodes) settings can be edited with one of the below commands:
    • List clusters with: kops get cluster
    • Edit this cluster with: kops edit cluster  awspoc.tk
    • Edit your node instance group: kops edit ig --name= awspoc.tk nodes
    • Edit your master instance group: kops edit ig --name= awspoc.tk master-us-east-1d
Run the commands as needed to make changes to the master and Node components. Wait for some time as it takes some time for the instances to boot and the DNS entries to be added in the hosted zone. Once everything is up; the Kubernetes nodes can be seen in the console. All these command should be run from the work station instance (i.e. Ubuntu box on which aws-cli and kubectl is setup):
The cluster settings can be edited with one of these commands:
    • List clusters with: 
$ kops get cluster
    • Edit this cluster with: 
$ kops edit cluster  awspoc.tk


To edit the configuration of worker nodes;
To increase or decrease our maximum size and minimum size of our nodes we can use commands:-
Edit your node instance group:
 $ kops edit ig nodes –name=awspoc.tk
 
       Note:
nodes=the name of our worker nodes
    	awspoc.tk=the name our cluster


To edit the configuration of master node;
 i.e. To increase or decrease our maximum size and minimum size of master node, use commands: -
Edit your master instance group: 
$ kops edit ig master-us-east-1d --name=awspoc.tk
Note:
master-us-east-1c=the name of our master node 
awspoc.tk=the name our cluster


NOTE: 
To stop the cluster without deleting the cluster;
Use the above mentioned command to stop the cluster by reducing the maximum and minimum size to zero in both master as well as worker nodes.

Wait for some time; as it takes some time for the instances to boot and the DNS entries to be added in the hosted zone. Once everything is up, the Kubernetes nodes can be viewed. 
Observation: The master and worker nodes cane be seen in AWS console as below:-
